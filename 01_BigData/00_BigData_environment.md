# ğŸš€ğŸš€ğŸš€å¤§æ•°æ®é›†ç¾¤é…ç½®ğŸš€ğŸš€ğŸš€

[TOC]



# 1.0 ğŸ›¹Hadoopå®‰è£…

![1659765478218](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1659765478218.png)

![1659766068823](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1659766068823.png)

## 1.1å…‹éš†è™šæ‹Ÿæœº102,103,104

**ï¼ˆ1ï¼‰ä¿®æ”¹å…‹éš†è™šæ‹Ÿæœºçš„é™æ€IP**

```
vim /etc/sysconfig/network-scripts/ifcfg-ens33
```

**ï¼ˆ2ï¼‰ä¿®æ”¹ä¸»æœºå**

```
vim /etc/hostname
```

**ï¼ˆ3ï¼‰è¿æ¥xshellå’ŒXFTP**

## 1.2ä¼ªåˆ†å¸ƒå¼çš„æµ‹è¯•

![1659766135778](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1659766135778.png)

**ï¼ˆ1ï¼‰å°†ä¸‹é¢å…­ä¸ªæ–‡ä»¶æ”¾å…¥/opt/softwareç›®å½•ä¸‹**

```
 apache-flume-1.9.0-bin.tar.gz
 
 apache-zookeeper-3.5.7-bin.tar.gz
 
 hadoop-3.1.3.tar.gz
 
 hbase-2.4.11-bin.tar.gz
 
 jdk-8u212-linux-x64.tar.gz
 
 kafka_2.12-3.0.0.tgz
```



**ï¼ˆ2ï¼‰å°†JDKå’ŒHadoopä¸¤ä¸ªæ–‡ä»¶åˆ†åˆ«è§£å‹**

```
tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
```

```
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
```



**ï¼ˆ3ï¼‰æ·»åŠ JDKå’ŒHadoopçš„ç¯å¢ƒå˜é‡**

```
sudo vim /etc/profile.d/my_env.sh
```

```
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin

#HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

æœ€åè®©æ–‡ä»¶ç”Ÿæ•ˆï¼š
source /etc/profile
```

**ï¼ˆ4ï¼‰å¯¹è™šæ‹Ÿæœºè¿›è¡Œé‡å¯å¹¶éªŒè¯hadoopå’ŒJDKæ˜¯å¦å®‰è£…æˆåŠŸ**

```
java -version
hadoop version Hadoop 3.1.3
```

**å¯ä»¥çœ‹åˆ°**

```
Hadoop 3.1.3  
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579
Compiled by ztang on 2019-09-12T02:47Z
Compiled with protoc 2.5.0
From source with checksum ec785077c385118ac91aadde5ec9799
This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar

java version "1.8.0_212"
Java(TM) SE Runtime Environment (build 1.8.0_212-b10)
Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)
```

**==è·³è¿‡ä¼ªåˆ†å¸ƒå¼å¯åŠ¨==**

## 1.3å®Œå…¨åˆ†å¸ƒå¼æ­å»º

![1659767783263](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1659767783263.png)





**ï¼ˆ1ï¼‰é¦–å…ˆè¿›è¡Œsshå…å¯†ç™»å½•**

```
ç”Ÿæˆå…¬é’¥å’Œç§é’¥
[atguigu@hadoop102 .ssh]$ pwd
/home/atguigu/.ssh

[atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa
ç„¶åæ•²ï¼ˆä¸‰ä¸ªå›è½¦ï¼‰ï¼Œå°±ä¼šç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶id_rsaï¼ˆç§é’¥ï¼‰ã€id_rsa.pubï¼ˆå…¬é’¥ï¼‰

å°†å…¬é’¥æ‹·è´åˆ°è¦å…å¯†ç™»å½•çš„ç›®æ ‡æœºå™¨ä¸Šï¼ˆæ¯ä¸ªæœºå™¨ä¸Šå‡æ‰§è¡Œä»¥ä¸‹ä¸‰ä¸ªå‘½ä»¤ï¼‰
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103
[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104

```

**(2)æµ‹è¯•æˆåŠŸåè¿›è¡Œxsyncé…ç½®**

```
ï¼ˆaï¼‰åœ¨/home/atguigu/binç›®å½•ä¸‹åˆ›å»ºxsyncæ–‡ä»¶
[atguigu@hadoop102 opt]$ cd /home/atguigu
[atguigu@hadoop102 ~]$ mkdir bin
[atguigu@hadoop102 ~]$ cd bin
[atguigu@hadoop102 bin]$ vim xsync
åœ¨è¯¥æ–‡ä»¶ä¸­ç¼–å†™å¦‚ä¸‹ä»£ç 

#!/bin/bash

#1. åˆ¤æ–­å‚æ•°ä¸ªæ•°
if [ $# -lt 1 ]
then
    echo Not Enough Arguement!
    exit;
fi

#2. éå†é›†ç¾¤æ‰€æœ‰æœºå™¨
for host in hadoop102 hadoop103 hadoop104
do
    echo ====================  $host  ====================
    #3. éå†æ‰€æœ‰ç›®å½•ï¼ŒæŒ¨ä¸ªå‘é€

    for file in $@
    do
        #4. åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if [ -e $file ]
            then
                #5. è·å–çˆ¶ç›®å½•
                pdir=$(cd -P $(dirname $file); pwd)

                #6. è·å–å½“å‰æ–‡ä»¶çš„åç§°
                fname=$(basename $file)
                ssh $host "mkdir -p $pdir"
                rsync -av $pdir/$fname $host:$pdir
            else
                echo $file does not exists!
        fi
    done
done

ï¼ˆbï¼‰ä¿®æ”¹è„šæœ¬ xsync å…·æœ‰æ‰§è¡Œæƒé™
[atguigu@hadoop102 bin]$ chmod +x xsync

ï¼ˆcï¼‰æµ‹è¯•è„šæœ¬
[atguigu@hadoop102 ~]$ xsync /home/fang/bin

ï¼ˆdï¼‰å°†è„šæœ¬å¤åˆ¶åˆ°/binä¸­ï¼Œä»¥ä¾¿å…¨å±€è°ƒç”¨
[atguigu@hadoop102 bin]$ sudo cp xsync /bin/

ï¼ˆeï¼‰åŒæ­¥ç¯å¢ƒå˜é‡é…ç½®ï¼ˆrootæ‰€æœ‰è€…ï¼‰
[atguigu@hadoop102 ~]$ sudo ./bin/xsync /etc/profile.d/my_env.sh
æ³¨æ„ï¼šå¦‚æœç”¨äº†sudoï¼Œé‚£ä¹ˆxsyncä¸€å®šè¦ç»™å®ƒçš„è·¯å¾„è¡¥å…¨ã€‚

è®©ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
[atguigu@hadoop103 bin]$ source /etc/profile
[atguigu@hadoop104 opt]$ source /etc/profile
```

**ï¼ˆ3ï¼‰æ£€æŸ¥103 104 ä¸Šçš„ç¯å¢ƒå˜é‡æ˜¯å¦ç”Ÿæ•ˆï¼Œè‹¥æœæ²¡æœ‰ç”Ÿæ•ˆå¯ä»¥é‡å¯è™šæ‹Ÿæœºå†æ¬¡æ£€æŸ¥**

```
java -version
hadoop version Hadoop 3.1.3
```

## 1.4å¯¹é…ç½®æ–‡ä»¶è¿›è¡Œé…ç½®

![1659770217317](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1659770217317.png)



**ï¼ˆ1ï¼‰æ ¸å¿ƒé…ç½®æ–‡ä»¶**

```
[atguigu@hadoop102 ~]$ cd $HADOOP_HOME/etc/hadoop
[atguigu@hadoop102 hadoop]$ vim core-site.xml

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- æŒ‡å®šNameNodeçš„åœ°å€ -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop102:8020</value>
    </property>

    <!-- æŒ‡å®šhadoopæ•°æ®çš„å­˜å‚¨ç›®å½• -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
    </property>

    <!-- é…ç½®HDFSç½‘é¡µç™»å½•ä½¿ç”¨çš„é™æ€ç”¨æˆ·ä¸ºatguigu -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>fang</value>  
    </property>
</configuration>
```

**ï¼ˆ2ï¼‰HDFS é…ç½®æ–‡ä»¶**

```
[atguigu@hadoop102 hadoop]$ vim hdfs-site.xml

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- nn webç«¯è®¿é—®åœ°å€-->
	<property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop102:9870</value>
    </property>
	<!-- 2nn webç«¯è®¿é—®åœ°å€-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop104:9868</value>
    </property>
</configuration>
```

**ï¼ˆ3ï¼‰YARN é…ç½®æ–‡ä»¶**

```
[atguigu@hadoop102 hadoop]$ vim yarn-site.xml

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- æŒ‡å®šMRèµ°shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>

    <!-- æŒ‡å®šResourceManagerçš„åœ°å€-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop103</value>
    </property>

    <!-- ç¯å¢ƒå˜é‡çš„ç»§æ‰¿ -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
<!-- å¼€å¯æ—¥å¿—èšé›†åŠŸèƒ½ -->
<property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>
<!-- è®¾ç½®æ—¥å¿—èšé›†æœåŠ¡å™¨åœ°å€ -->
<property>  
    <name>yarn.log.server.url</name>  
    <value>http://hadoop102:19888/jobhistory/logs</value>
</property>
<!-- è®¾ç½®æ—¥å¿—ä¿ç•™æ—¶é—´ä¸º7å¤© -->
<property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
</property>
</configuration>
```

**ï¼ˆ4ï¼‰MapReduce é…ç½®æ–‡ä»¶**

```
[atguigu@hadoop102 hadoop]$ vim mapred-site.xml

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- æŒ‡å®šMapReduceç¨‹åºè¿è¡Œåœ¨Yarnä¸Š -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
<!-- å†å²æœåŠ¡å™¨ç«¯åœ°å€ -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop102:10020</value>
</property>

<!-- å†å²æœåŠ¡å™¨webç«¯åœ°å€ -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>
</configuration>
```

**ï¼ˆ5ï¼‰é…ç½® workers**

```
[atguigu@hadoop102 hadoop]$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers

hadoop102
hadoop103
hadoop104

åŒæ­¥æ‰€æœ‰é…ç½®
[atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc
```

**ï¼ˆ6ï¼‰åˆå§‹åŒ–NameNode**

```
[atguigu@hadoop102 hadoop-3.1.3]$ hdfs namenode -format
```

**ï¼ˆ7ï¼‰ç¼–å†™myhadoop.shè„šæœ¬**

```
[atguigu@hadoop102 ~]$ cd /home/fang/bin
[atguigu@hadoop102 bin]$ vim myhadoop.sh

#!/bin/bash

if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi

case $1 in
"start")
        echo " =================== å¯åŠ¨ hadoopé›†ç¾¤ ==================="

        echo " --------------- å¯åŠ¨ hdfs ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
        echo " --------------- å¯åŠ¨ yarn ---------------"
        ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
        echo " --------------- å¯åŠ¨ historyserver ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
;;
"stop")
        echo " =================== å…³é—­ hadoopé›†ç¾¤ ==================="

        echo " --------------- å…³é—­ historyserver ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
        echo " --------------- å…³é—­ yarn ---------------"
        ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
        echo " --------------- å…³é—­ hdfs ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
    echo "Input Args Error..."
;;
esac

è®°å¾—æ·»åŠ æ‰§è¡Œæƒé™
[atguigu@hadoop102 bin]$ chmod +x myhadoop.sh 
```

**ï¼ˆ8ï¼‰ç¼–å†™jpsallè„šæœ¬**

```
[atguigu@hadoop102 ~]$ cd /home/atguigu/bin
[atguigu@hadoop102 bin]$ vim jpsall

#!/bin/bash

for host in hadoop102 hadoop103 hadoop104
do
        echo =============== $host ===============
        ssh $host jps 
done

æ·»åŠ è„šæœ¬å¹¶åˆ†å‘è„šæœ¬
[atguigu@hadoop102 bin]$ chmod +x jpsall
[atguigu@hadoop102 ~]$ xsync /home/fang/bin/
åœ¨103 104ä¸Šæ·»åŠ æ‰§è¡Œæƒé™
```

ï¼ˆ9ï¼‰å¯åŠ¨é›†ç¾¤

```
myhadoop.sh start  å¯åŠ¨é›†ç¾¤

jpsall      æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹æ˜¯å¦å¯åŠ¨
```

(10)æŸ¥çœ‹3ä¸ªç½‘å€æ˜¯å¦å¯ä»¥æ­£å¸¸è®¿é—®

```
http://hadoop102:9870
http://hadoop103:8088/
http://hadoop102:19888/jobhistory
```

==å¦‚æœèŠ‚ç‚¹éƒ½æ­£å¸¸å¯åŠ¨ï¼Œé¡µé¢æ— æ³•è®¿é—®ï¼Œæ£€æŸ¥hostsæ–‡ä»¶æ˜¯å¦ä¿®æ”¹ï¼Œå¦‚æœæ— è¯¯ï¼Œå¯ä»¥é‡å†™å¯åŠ¨ï¼Œå¹¶è¿›è¡Œæµ‹è¯•==

```
echo $PATH
/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/root/.local/bin:/home/zxb/bin:/opt/module/jdk1.8.0_212/bin

echo $PATH
/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/root/.local/bin:/home/zxb/bin:/opt/module/jdk1.8.0_212/bin
```



# 2.0âœˆï¸ zookeeperå®‰è£…

## 2.1è§£å‹å®‰è£…

```
è§£å‹
[atguigu@hadoop102 software]$ tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/

æ›´å
[atguigu@hadoop102 module]$ mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7
```

## 2.2æ–‡ä»¶é…ç½®

**é…ç½®æœåŠ¡å™¨ç¼–å·**

ï¼ˆ1ï¼‰åœ¨/opt/module/zookeeper-3.5.7/è¿™ä¸ªç›®å½•ä¸‹åˆ›å»º zkData

```
[atguigu@hadoop102 zookeeper-3.5.7]$ mkdir zkData
```

ï¼ˆ2ï¼‰åœ¨/opt/module/zookeeper-3.5.7/zkData ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª myid çš„æ–‡ä»¶

```
[atguigu@hadoop102 zkData]$ vi myid

åœ¨æ–‡ä»¶ä¸­æ·»åŠ ä¸ server å¯¹åº”çš„ç¼–å·ï¼ˆæ³¨æ„ï¼šä¸Šä¸‹ä¸è¦æœ‰ç©ºè¡Œï¼Œå·¦å³ä¸è¦æœ‰ç©ºæ ¼ï¼‰

2

[atguigu@hadoop102 module ]$ xsync zookeeper-3.5.7

vim /opt/module/zookeeper-3.5.7/zkData/myid
å¹¶åˆ†åˆ«åœ¨ hadoop103ã€hadoop104 ä¸Šä¿®æ”¹ myid æ–‡ä»¶ä¸­å†…å®¹ä¸º 3ã€4
```

**ï¼ˆ3ï¼‰é…ç½®zoo.cfgæ–‡ä»¶**

```
ï¼ˆ1ï¼‰é‡å‘½å/opt/module/zookeeper-3.5.7/conf è¿™ä¸ªç›®å½•ä¸‹çš„ zoo_sample.cfg ä¸º zoo.cfg
[atguigu@hadoop102 conf]$ mv zoo_sample.cfg zoo.cfg

ï¼ˆ2ï¼‰æ‰“å¼€ zoo.cfg æ–‡ä»¶
[atguigu@hadoop102 conf]$ vim zoo.cfg

#ä¿®æ”¹æ•°æ®å­˜å‚¨è·¯å¾„é…ç½®
dataDir=/opt/module/zookeeper-3.5.7/zkData
#å¢åŠ å¦‚ä¸‹é…ç½®
#######################cluster##########################
server.2=hadoop102:2888:3888
server.3=hadoop103:2888:3888
server.4=hadoop104:2888:3888

ï¼ˆ3ï¼‰åŒæ­¥ zoo.cfg é…ç½®æ–‡ä»¶
[atguigu@hadoop102 conf]$ xsync zoo.cfg
```

**ï¼ˆ4ï¼‰ç¼–å†™å¯åœè„šæœ¬**

```
1ï¼‰åœ¨ hadoop102 çš„/home/atguigu/bin ç›®å½•ä¸‹åˆ›å»ºè„šæœ¬
[atguigu@hadoop102 bin]$ vim zk.sh

#!/bin/bash
case $1 in
"start"){
	for i in hadoop102 hadoop103 hadoop104
	do
        echo ---------- zookeeper $i å¯åŠ¨ ------------
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in hadoop102 hadoop103 hadoop104
	do
        echo ---------- zookeeper $i åœæ­¢ ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in hadoop102 hadoop103 hadoop104
	do
        echo ---------- zookeeper $i çŠ¶æ€ ------------    
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
	done
};;
esac

2ï¼‰å¢åŠ è„šæœ¬æ‰§è¡Œæƒé™
[atguigu@hadoop102 bin]$ chmod u+x zk.sh

3ï¼‰Zookeeper é›†ç¾¤å¯åŠ¨è„šæœ¬
[atguigu@hadoop102 module]$ zk.sh start

4ï¼‰Zookeeper é›†ç¾¤åœæ­¢è„šæœ¬
[atguigu@hadoop102 module]$ zk.sh stop

è¾“å…¥jpsallæ­£å¸¸å¯åŠ¨å³å¯
```

**ï¼ˆ5ï¼‰zookeeperçš„æ“ä½œ**

```
å¯åŠ¨zookeeperå®¢æˆ·ç«¯
[atguigu@hadoop102 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop102:2181

æŸ¥çœ‹zookeeperèŠ‚ç‚¹ä¿¡æ¯
[zk: hadoop102:2181(CONNECTED) 0] ls /
```

# 3.0ğŸˆHBaseå®‰è£…

â€‹		HBase é€šè¿‡ Zookeeper æ¥åš master çš„é«˜å¯ç”¨ã€è®°å½• RegionServer çš„éƒ¨ç½²ä¿¡æ¯ã€å¹¶ä¸”å­˜å‚¨æœ‰ meta è¡¨çš„ä½ç½®ä¿¡æ¯ã€‚ 

â€‹		HBase å¯¹äºæ•°æ®çš„è¯»å†™æ“ä½œæ—¶ç›´æ¥è®¿é—® Zookeeper çš„ï¼Œåœ¨ 2.3 ç‰ˆæœ¬æ¨å‡º Master Registry æ¨¡å¼ï¼Œå®¢æˆ·ç«¯å¯ä»¥ç›´æ¥è®¿é—® masterã€‚ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œä¼šåŠ å¤§å¯¹ master çš„å‹åŠ›ï¼Œå‡è½»å¯¹ Zookeeperçš„å‹åŠ›ã€‚



## 3.1è§£å‹å®‰è£…ã€ç¯å¢ƒå˜é‡

```
1ï¼‰è§£å‹ Hbase åˆ°æŒ‡å®šç›®å½•
[atguigu@hadoop102 software]$ tar -zxvf hbase-2.4.11-bin.tar.gz -C /opt/module/
[atguigu@hadoop102 software]$ mv /opt/module/hbase-2.4.11 /opt/module/hbase

2ï¼‰é…ç½®ç¯å¢ƒå˜é‡
[atguigu@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh

æ·»åŠ 
#HBASE_HOME
export HBASE_HOME=/opt/module/hbase
export PATH=$PATH:$HBASE_HOME/bin

3ï¼‰ä½¿ç”¨ source è®©é…ç½®çš„ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
[atguigu@hadoop102 module]$ source /etc/profile.d/my_env.sh

[fang@hadoop102 hbase]$ sudo /home/fang/bin/xsync /etc/profile.d/my_env.sh 

```



## 3.2æ–‡ä»¶é…ç½®

1ï¼‰hbase-env.sh ä¿®æ”¹å†…å®¹ï¼Œå¯ä»¥æ·»åŠ åˆ°æœ€åï¼š

åœ¨hbase/confç›®å½•ä¸‹

```
[fang@hadoop102 conf]$ vim hbase-env.sh

æœ€åé¢æ·»åŠ ä¸€å¥
export HBASE_MANAGES_ZK=false

```

2ï¼‰hbase-site.xml ä¿®æ”¹å†…å®¹ï¼š ==æŠŠåŸå…ˆæ ‡ç­¾å†…çš„å†…å®¹åˆ æ‰ï¼Œç›´æ¥æ›¿æ¢æˆä¸‹æ–¹çš„å³å¯==

```
<property>
  <name>hbase.cluster.distributed</name>
  <value>true</value>
</property>

<property>
 <name>hbase.zookeeper.quorum</name>
 <value>hadoop102,hadoop103,hadoop104</value>
 <description>The directory shared by RegionServers.
 </description>
 </property>
 
<!-- <property>-->
<!-- <name>hbase.zookeeper.property.dataDir</name>-->
<!-- <value>/export/zookeeper</value>-->
<!-- <description> è®°å¾—ä¿®æ”¹ ZK çš„é…ç½®æ–‡ä»¶ -->
<!-- ZK çš„ä¿¡æ¯ä¸èƒ½ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹-->
<!-- </description>-->
<!-- </property>-->

 <property>
 <name>hbase.rootdir</name>
 <value>hdfs://hadoop102:8020/hbase</value>
 <description>The directory shared by RegionServers.
 </description>
 </property>
 
 <property>
 <name>hbase.cluster.distributed</name>
 <value>true</value>
 </property>
```

3ï¼‰ä¿®æ”¹regionservers

```
hadoop102 
hadoop103 
hadoop104 
```

4ï¼‰è§£å†³ HBase å’Œ Hadoop çš„ log4j å…¼å®¹æ€§é—®é¢˜ï¼Œä¿®æ”¹ HBase çš„ jar åŒ…ï¼Œä½¿ç”¨ Hadoop çš„ jar åŒ…

```
[atguigu@hadoop102hbase]$mv /opt/module/hbase/lib/client-facing-thirdparty/slf4j-reload4j-1.7.33.jar /opt/module/hbase/lib/client-facing-thirdparty/slf4j-reload4j-1.7.33.jar.bak




å‘é€åˆ°å…¶ä»–
xsync hbase/
```

5ï¼‰Hbaseå¯åœ

```
[atguigu@hadoop102 hbase]$ bin/start-hbase.sh
[atguigu@hadoop102 hbase]$ bin/stop-hbase.sh
```



# 4.0 ğŸ°Flumeå®‰è£…

## 4.1è§£å‹å®‰è£…

```
è§£å‹
[atguigu@hadoop102 software]$ tar -zxf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/

æ”¹å
[atguigu@hadoop102 module]$ mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume

å°† lib æ–‡ä»¶å¤¹ä¸‹çš„ guava-11.0.2.jar åˆ é™¤ä»¥å…¼å®¹ Hadoop 3.1.3
[atguigu@hadoop102 lib]$ rm /opt/module/flume/lib/guava-11.0.2.jar
```

## 4.2 é…ç½®

```
ï¼ˆ1ï¼‰å®‰è£… netcat å·¥å…·
[atguigu@hadoop102 software]$ sudo yum install -y nc

ï¼ˆ2ï¼‰åˆ¤æ–­ 44444 ç«¯å£æ˜¯å¦è¢«å ç”¨
[atguigu@hadoop102 flume-telnet]$ sudo netstat -nlp | grep 44444

ï¼ˆ3ï¼‰åˆ›å»º Flume Agent é…ç½®æ–‡ä»¶ flume-netcat-logger.conf
ï¼ˆ4ï¼‰åœ¨ flume ç›®å½•ä¸‹åˆ›å»º job æ–‡ä»¶å¤¹å¹¶è¿›å…¥ job æ–‡ä»¶å¤¹ã€‚
[atguigu@hadoop102 flume]$ mkdir job
[atguigu@hadoop102 flume]$ cd job/

ï¼ˆ5ï¼‰åœ¨ job æ–‡ä»¶å¤¹ä¸‹åˆ›å»º Flume Agent é…ç½®æ–‡ä»¶ flume-netcat-logger.confã€‚
[atguigu@hadoop102 job]$ vim flume-netcat-logger.conf

ï¼ˆ6ï¼‰åœ¨ flume-netcat-logger.conf æ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹ã€‚
æ·»åŠ å†…å®¹å¦‚ä¸‹ï¼š
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444
# Describe the sink
a1.sinks.k1.type = logger
# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

ï¼ˆ7ï¼‰å…ˆå¼€å¯ flume ç›‘å¬ç«¯å£
[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console

å¤åˆ¶102ä¼šè¯

ï¼ˆ8ï¼‰ä½¿ç”¨ netcat å·¥å…·å‘æœ¬æœºçš„ 44444 ç«¯å£å‘é€å†…å®¹
[atguigu@hadoop102 ~]$ nc localhost 44444

```

==åˆ°è¿™å°±å¯ä»¥äº†ï¼Œé¡¹ç›®ä¸­ä¼šç»§ç»­è¿›è¡Œé…ç½®==

# 5.0 ğŸ˜—Kafkaå®‰è£…

## 5.1è§£å‹å®‰è£…é…ç½®ç¯å¢ƒå˜é‡

```
è§£å‹
[atguigu@hadoop102 software]$ tar -zxvf kafka_2.12-3.0.0.tgz -C /opt/module/

æ”¹å
[atguigu@hadoop102 module]$ mv kafka_2.12-3.0.0/ kafka

è¿›å…¥åˆ°/opt/module/kafka ç›®å½•ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶
[atguigu@hadoop102 kafka]$ cd config/
[atguigu@hadoop102 config]$ vim server.properties

ä¿®æ”¹ä¸‰ä¸ªåœ°æ–¹ï¼šè®°å¾—æ‰¾å¯¹ç›¸åº”çš„ä½ç½®

#broker çš„å…¨å±€å”¯ä¸€ç¼–å·ï¼Œä¸èƒ½é‡å¤ï¼Œåªèƒ½æ˜¯æ•°å­—ã€‚
broker.id=0

#kafka è¿è¡Œæ—¥å¿—(æ•°æ®)å­˜æ”¾çš„è·¯å¾„ï¼Œè·¯å¾„ä¸éœ€è¦æå‰åˆ›å»ºï¼Œkafka è‡ªåŠ¨å¸®ä½ åˆ›å»ºï¼Œå¯ä»¥
é…ç½®å¤šä¸ªç£ç›˜è·¯å¾„ï¼Œè·¯å¾„ä¸è·¯å¾„ä¹‹é—´å¯ä»¥ç”¨"ï¼Œ"åˆ†éš”
log.dirs=/opt/module/kafka/datas


# æ£€æŸ¥è¿‡æœŸæ•°æ®çš„æ—¶é—´ï¼Œé»˜è®¤ 5 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æ•°æ®è¿‡æœŸ
log.retention.check.interval.ms=300000
#é…ç½®è¿æ¥ Zookeeper é›†ç¾¤åœ°å€ï¼ˆåœ¨ zk æ ¹ç›®å½•ä¸‹åˆ›å»º/kafkaï¼Œæ–¹ä¾¿ç®¡ç†ï¼‰
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka

åˆ†å‘
[atguigu@hadoop102 module]$ xsync kafka/

åˆ†åˆ«åœ¨ hadoop103 å’Œ hadoop104 ä¸Šä¿®æ”¹é…ç½®æ–‡ä»¶
[fang@hadoop102 flume]$ vim /opt/module/kafka/config/server.properties 
ä¸­çš„ broker.id=1ã€broker.id=2

åœ¨/etc/profile.d/my_env.sh æ–‡ä»¶ä¸­å¢åŠ  kafka ç¯å¢ƒå˜é‡é…ç½®
[atguigu@hadoop102 module]$ sudo vim /etc/profile.d/my_env.sh

#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin
åˆ·æ–°
[atguigu@hadoop102 module]$ source /etc/profile

åˆ†å‘å¹¶åˆ·æ–°
[atguigu@hadoop102 module]$ sudo /home/fang/bin/xsync /etc/profile.d/my_env.sh
[atguigu@hadoop103 module]$ source /etc/profile
[atguigu@hadoop104 module]$ source /etc/profile
```

## 5.2é›†ç¾¤å¯åŠ¨

```
ï¼ˆ1ï¼‰å…ˆå¯åŠ¨ Zookeeper é›†ç¾¤ï¼Œç„¶åå¯åŠ¨ Kafkaã€‚
[atguigu@hadoop102 kafka]$ zk.sh start

ï¼ˆ2ï¼‰é›†ç¾¤è„šæœ¬
1ï¼‰åœ¨/home/atguigu/bin ç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶ kf.sh è„šæœ¬æ–‡ä»¶
[atguigu@hadoop102 bin]$ vim kf.sh


#! /bin/bash
case $1 in
        "start" ){
        for i in hadoop102 hadoop103 hadoop104;
        do
                echo "==============$i start=============="
                ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
        done
};;
        "stop" ){
        for i in hadoop102 hadoop103 hadoop104;
        do
                echo "==============$i stop=============="
                ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh"
        done
};;
esac


2ï¼‰æ·»åŠ æ‰§è¡Œæƒé™
[atguigu@hadoop102 bin]$ chmod +x kf.sh
3ï¼‰å¯åŠ¨é›†ç¾¤å‘½ä»¤
[atguigu@hadoop102 ~]$ kf.sh start
4ï¼‰åœæ­¢é›†ç¾¤å‘½ä»¤
[atguigu@hadoop102 ~]$ kf.sh stop


```

# 6.0ğŸŒRedisç¯å¢ƒé…ç½®

**1.å°†è½¯ä»¶åŒ…ä¸Šä¼ åˆ°/opt/softwereç›®å½•ä¸‹ï¼Œè§£å‹åˆ°/opt/modulä¸‹**

```
[root@linux softwere]# tar -zxvf redis-3.0.0.tar.gz -C /opt/module/
```

**2.è¿›å…¥redisç›®å½•è¿›è¡Œå¯åŠ¨**

```
[root@linux redis-3.0.0]# make
```

==**å‡ºç°å¦‚ä¸‹æŠ¥é”™ï¼šæ²¡æœ‰æ‰¾åˆ°gccï¼Œæ­¤æ—¶éœ€è¦å®‰è£…gcc**==

```
[root@linux module]# yum -y install gcc
```

![1660718220098](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660718220098.png)

**å®‰è£…å®ŒæˆåéªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸ**

```
[root@linux module]# gcc -v
```

![1660721470107](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660721470107.png)

**3.å†æ¬¡è¿›è¡Œmakeå‡ºç°å¦‚ä¸‹æŠ¥é”™**

![1660721564091](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660721564091.png)

æ­¤æ—¶æ‰§è¡Œ

```
#æ¸…ç†æ®‹ä½™æ–‡ä»¶
make distclean

#å†æ¬¡è¿›è¡Œmake
make
```

æ‰§è¡ŒæˆåŠŸ

![1660721704391](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660721704391.png)

**4.æ‰§è¡Œ**

```
make install
```

![1660721859955](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660721859955.png)

**5.æŸ¥çœ‹å®‰è£…ç›®å½•**

```
[root@linux redis-3.0.0]# cd /usr/local/bin/
```

![1660722008875](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660722008875.png)

6.å›åˆ°Redisç›®å½•å¤‡ä»½æ–‡ä»¶

```
#åœ¨æ ¹ç›®å½•åˆ›å»ºæ–‡ä»¶å¤¹
[root@linux /]# mkdir myredis

#å°†redis.confç§»åŠ¨åˆ°æ–‡ä»¶å¤¹æ±‡æ€»
[root@linux redis-3.0.0]# cp redis.conf /myredis/

#åœ¨myredisç›®å½•ä¸­ç¼–è¾‘æ–‡ä»¶
[root@linux myredis]# vi redis.conf 
```

![1660722373407](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660722373407.png)

```
#åˆ‡æ¢ç›®å½•
[root@linux myredis]# cd /usr/local/bin/

#å¯åŠ¨redis
[root@linux bin]# redis-server /myredis/redis.conf 

#æŸ¥çœ‹å®¢æˆ·ç«¯
[root@linux bin]# redis-cli -p 6379

```

![1660722559207](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660722559207.png)

**æµ‹è¯•**

![1660722595954](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660722595954.png)

æŸ¥çœ‹redisåœ¨åå°æ˜¯å¦å¯åŠ¨

```
[root@linux bin]# ps -ef|grep redis
```

![1660722694207](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660722694207.png)

# 7.0 ğŸšœMySqlå®‰è£…

**1ï¼‰æ£€æŸ¥å½“å‰ç³»ç»Ÿæ˜¯å¦å®‰è£…è¿‡ MySQL**

```
[atguigu@hadoop102 ~]$ rpm -qa|grep mariadb
mariadb-libs-5.5.56-2.el7.x86_64 
//å¦‚æœå­˜åœ¨é€šè¿‡å¦‚ä¸‹å‘½ä»¤å¸è½½
[atguigu @hadoop102 ~]$ sudo rpm -e --nodeps mariadb-libs
```

**2ï¼‰å°† MySQL å®‰è£…åŒ…æ‹·è´åˆ°/opt/software ç›®å½•ä¸‹**

```
[atguigu @hadoop102 software]# ll
æ€»ç”¨é‡ 528384
-rw-r--r--. 1 root root 609556480 3 æœˆ 21 15:41 mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar
```

**3ï¼‰è§£å‹ MySQL å®‰è£…åŒ…**

```
[atguigu @hadoop102 software]# tar -xf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar
```

**4ï¼‰åœ¨å®‰è£…ç›®å½•ä¸‹æ‰§è¡Œ rpm å®‰è£…**

```
[atguigu @hadoop102 software]$ 
sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm
sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm
sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm
sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm
sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm
```

â€‹		==è¿‡ç¨‹ä¸­å‡ºç°æŠ¥é”™åˆ™è¯´æ˜éœ€è¦å®‰è£…ä¾èµ–ï¼Œé€šè¿‡ yum å®‰è£…ç¼ºå°‘çš„ä¾èµ–,ç„¶åé‡æ–°å®‰è£… mysql-community-server-5.7.28-1.el7.x86_64 å³å¯==cd

```
[atguigu@hadoop102 software] yum install -y libaio
```

**5ï¼‰åˆ é™¤/etc/my.cnf æ–‡ä»¶ä¸­ datadir æŒ‡å‘çš„ç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹,å¦‚æœæœ‰å†…å®¹çš„æƒ…å†µä¸‹:**

 **æŸ¥çœ‹ datadir çš„å€¼ï¼š** 

```
[mysqld]
datadir=/var/lib/mysql
```

**åˆ é™¤/var/lib/mysql ç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹ï¼ˆä¸€èˆ¬æ²¡æœ‰ä»»ä½•ä¸œè¥¿ï¼‰:**

```
[atguigu @hadoop102 mysql]# cd /var/lib/mysql
[atguigu @hadoop102 mysql]# sudo rm -rf ./* //æ³¨æ„æ‰§è¡Œå‘½ä»¤çš„ä½ç½®
```

**6ï¼‰åˆå§‹åŒ–æ•°æ®åº“**

```
[atguigu @hadoop102 opt]$ sudo mysqld --initialize --user=mysql
```

**7ï¼‰æŸ¥çœ‹ä¸´æ—¶ç”Ÿæˆçš„ root ç”¨æˆ·çš„å¯†ç **

```
[atguigu @hadoop102 opt]$ sudo cat /var/log/mysqld.log       jZu%i>4hD
```

![1660702382426](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660702382426.png)

**8ï¼‰å¯åŠ¨ MySQL æœåŠ¡**

```
[atguigu @hadoop102 opt]$ sudo systemctl start mysqld
```

**9ï¼‰ç™»å½• MySQL æ•°æ®åº“**

```
[atguigu @hadoop102 opt]$ mysql -uroot -p
Enter password: è¾“å…¥ä¸´æ—¶ç”Ÿæˆçš„å¯†ç 
```

**10ï¼‰å¿…é¡»å…ˆä¿®æ”¹ root ç”¨æˆ·çš„å¯†ç ,å¦åˆ™æ‰§è¡Œå…¶ä»–çš„æ“ä½œä¼šæŠ¥é”™**

```
mysql> set password = password("fgl123");
```

**11ï¼‰ä¿®æ”¹ mysql åº“ä¸‹çš„ user è¡¨ä¸­çš„ root ç”¨æˆ·å…è®¸ä»»æ„ ip è¿æ¥**

```
mysql> update mysql.user set host='%' where user='root';
mysql> flush privileges;
```

**12ï¼‰ä½¿ç”¨æœ¬æœºNavicatè¿æ¥è¿œç¨‹æ•°æ®åº“**

![1660702611954](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660702611954.png)

# 8.0âš½Tomcatå®‰è£…

**ï¼ˆ1ï¼‰ä¸Šä¼ Tomcatæ–‡ä»¶åŒ…åˆ°/opt/softwere**

```
apache-tomcat-8.5.75 
```

**ï¼ˆ2ï¼‰è§£å‹Tomacat**

```
tar -zxvf apache-tomcat-8.5.75 
mv  apache-tomcat-8.5.75 /opt/module
```

### 8.8.1å¦‚ä½•å°†é¡¹ç›®æ‰“waråŒ…

**1.åœ¨IDEAä¸­æ‰“å¼€Project Structure**

![1660705598516](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660705598516.png)

**2.é€‰æ‹©**

![1660705637100](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660705637100.png)

**3.ç‚¹å‡»+å·**

![1660705666763](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660705666763.png)

**4.é€‰æ‹©**

![1660705700030](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660705700030.png)

**5.é…ç½®å¹¶æ‰“åŒ…**

![1660705778126](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660705778126.png)

![1660705807161](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660705807161.png)

![1660705831419](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660705831419.png)

### 8.8.2å¦‚ä½•åœ¨æœ¬åœ°Tomcatè¿è¡ŒwaråŒ…

**1.å°†waråŒ…æ”¾å…¥æœ¬åœ°Tomcatç›®å½•ä¸­çš„webappä¸‹**

![1660706036288](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660706036288.png)

**2.åœ¨tomcat  binç›®å½•ä¸‹æ‰“å¼€ç»ˆç«¯è¾“å…¥.\startup.bat**

![1660706169703](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660706169703.png)

**3.å¼¹å‡ºè¿è¡Œæ¡†æ­£å¸¸è¿è¡Œå³å¯è®¿é—®webé¡¹ç›®**

![1660706234298](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660706234298.png)

**4.å…³é—­Tomcat**

![1660706297352](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1660706297352.png)





# 9.0 ğŸ¥°Hiveçš„å®‰è£…

## 9.1Hiveå®‰è£…éƒ¨ç½²

### 9.1.1Hiveå®‰è£…éƒ¨ç½²

ï¼ˆ1ï¼‰æŠŠapache-hive-3.1.2-bin.tar.gzä¸Šä¼ åˆ°Linuxçš„/opt/softwareç›®å½•ä¸‹

ï¼ˆ2ï¼‰è§£å‹apache-hive-3.1.2-bin.tar.gzåˆ°/opt/module/ç›®å½•ä¸‹é¢

```
[atguigu@hadoop102 software]$ tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/
```

ï¼ˆ3ï¼‰ä¿®æ”¹apache-hive-3.1.2-bin.tar.gzçš„åç§°ä¸ºhive

```
[atguigu@hadoop102 software]$ mv /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive
```

ï¼ˆ4ï¼‰ä¿®æ”¹/etc/profile.d/my_env.shï¼Œæ·»åŠ ç¯å¢ƒå˜é‡

```
[atguigu@hadoop102 software]$ sudo vim /etc/profile.d/my_env.sh
```

æ·»åŠ å†…å®¹

```
#HIVE_HOME
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
```

é‡å¯Xshellå¯¹è¯æ¡†æˆ–è€…sourceä¸€ä¸‹ /etc/profile.d/my_env.shæ–‡ä»¶ï¼Œä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ

```
[atguigu@hadoop102 software]$ source /etc/profile.d/my_env.sh
```

ï¼ˆ5ï¼‰è§£å†³æ—¥å¿—JaråŒ…å†²çªï¼Œè¿›å…¥/opt/module/hive/libç›®å½•

```
[atguigu@hadoop102 lib]$ mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak
```



## 9.2Hiveå…ƒæ•°æ®é…ç½®åˆ°Mysql

### 9.2.1æ‹·è´é©±åŠ¨

å°†MySQLçš„JDBCé©±åŠ¨æ‹·è´åˆ°Hiveçš„libç›®å½•ä¸‹

```
[atguigu@hadoop102 lib]$ cp /opt/software/mysql-connector-java-5.1.27.jar /opt/module/hive/lib/
```

### 9.2.2é…ç½®Metastoreåˆ°Mysql

ï¼ˆ1ï¼‰åœ¨$HIVE_HOME/confç›®å½•ä¸‹æ–°å»ºhive-site.xmlæ–‡ä»¶

```
[atguigu@hadoop102 conf]$ vim hive-site.xml
```

ï¼ˆ2ï¼‰æ·»åŠ å¦‚ä¸‹å†…å®¹

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>fgl123</value>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>

    <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
    </property>

    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>hadoop102</value>
    </property>

    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>
    
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
</configuration>
```

## 9.3å¯åŠ¨Hive

### 9.3.1åˆå§‹åŒ–æºæ•°æ®åº“

ï¼ˆ1ï¼‰ç™»é™†MySQL

```
[atguigu@hadoop102 conf]$ mysql -uroot -p000000
```

ï¼ˆ2ï¼‰æ–°å»ºHiveå…ƒæ•°æ®åº“

```
mysql> create database metastore;
mysql> quit;
```

ï¼ˆ3ï¼‰åˆå§‹åŒ–Hiveå…ƒæ•°æ®åº“

```
[atguigu@hadoop102 conf]$ schematool -initSchema -dbType mysql -verbose
```

### 9.3.2å¯åŠ¨Hiveå®¢æˆ·ç«¯

ï¼ˆ1ï¼‰å¯åŠ¨Hiveå®¢æˆ·ç«¯

```
[atguigu@hadoop102 hive]$ bin/hive
```

ï¼ˆ2ï¼‰æŸ¥çœ‹ä¸€ä¸‹æ•°æ®åº“

```
hive (default)> show databases;
OK
database_name
default
```



**é›†ç¾¤ç¯å¢ƒæŸ¥çœ‹**

æ­¤æ—¶è¾“å…¥ï¼šjpsall  å¯æŸ¥çœ‹åˆ°ä¸€ä¸‹èŠ‚ç‚¹ç›¸åº”ä¿¡æ¯

|       è½¯ä»¶       |    hadoop102     |    hadoop103    |     hadoop104     |
| :--------------: | :--------------: | :-------------: | :---------------: |
|   Hadoop  HDFS   |     NameNode     |                 | SecondaryNameNode |
|                  |     DataNode     |    DataNode     |     DataNode      |
|   Hadoop  YARN   |                  | ResourceManager |                   |
|                  |   NodeManager    |   NodeManager   |    NodeManager    |
| Hadoopå†å²æœåŠ¡å™¨ | JobHistoryServer |                 |                   |
|    Zookeeper     |  QuorumPeerMain  | QuorumPeerMain  |  QuorumPeerMain   |
|      Kafka       |      Kafka       |      Kafka      |       Kafka       |
|      HBase       |     HMaster      |                 |                   |
|                  |  HRegionServer   |  HRegionServer  |   HRegionServer   |
|                  |       jsp        |       jsp       |        jsp        |

**é›†ç¾¤åœæ­¢è„šæœ¬ï¼š**

```
Hadoopå¯åœï¼š
myhadoop.sh start
myhadoop.sh stop

HBaseå¯åœï¼š
bin/start-hbase.sh
bin/stop-hbase.sh

Zookeeperå¯åœï¼š å…ˆå¯åœZookeeperå†å¯åœKafka
zk.sh start
zk.sh stop

KafKaå¯åœï¼š
kf.sh start
kf.sh stop

å„å¤§ç½‘å€ï¼š
http://hadoop102:9870
http://hadoop103:8088/
http://hadoop102:19888/jobhistory
http://hadoop102:16010/master-status
```

==æ³¨æ„ï¼š==

==Hbaseå¯åŠ¨å‰å…ˆå¯åŠ¨zookeeperå†å¯åŠ¨Hadoop==

==å¿…é¡»å…ˆå…³é—­KafKaå†å…³é—­Zookeeper==

10.0

# 10.0ğŸš€Sqoopå®‰è£…

## 10.1 ä¸‹è½½å¹¶è§£å‹	

1ï¼‰sqoopå®˜ç½‘åœ°å€ï¼š[http://sqoop.apache.org](http://sqoop.apache.org/docs/1.4.7/index.html)

2ï¼‰ä¸‹è½½åœ°å€ï¼šhttp://mirrors.hust.edu.cn/apache/sqoop/1.4.6/

3ï¼‰ä¸Šä¼ å®‰è£…åŒ…sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gzåˆ°hadoop102çš„/opt/softwareè·¯å¾„ä¸­

4ï¼‰è§£å‹sqoopå®‰è£…åŒ…åˆ°æŒ‡å®šç›®å½•ï¼Œå¦‚ï¼š

```
[atguigu@hadoop102 software]$ tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/
```

5ï¼‰è§£å‹sqoopå®‰è£…åŒ…åˆ°æŒ‡å®šç›®å½•ï¼Œå¦‚ï¼š

```
[atguigu@hadoop102 module]$ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop
```

## 10.2 ä¿®æ”¹é…ç½®æ–‡ä»¶	

1ï¼‰è¿›å…¥åˆ°/opt/module/sqoop/confç›®å½•ï¼Œé‡å‘½åé…ç½®æ–‡ä»¶

```
[atguigu@hadoop102 conf]$ mv sqoop-env-template.sh sqoop-env.sh
```

2ï¼‰ä¿®æ”¹é…ç½®æ–‡ä»¶

```
[atguigu@hadoop102 conf]$ vim sqoop-env.sh 
```

å¢åŠ å¦‚ä¸‹å†…å®¹

```
export HADOOP_COMMON_HOME=/opt/module/hadoop-3.1.3
export HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3
export HIVE_HOME=/opt/module/hive
export ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7
export ZOOCFGDIR=/opt/module/zookeeper-3.5.7/conf
```

## 10.3 æ‹·è´JDBCé©±åŠ¨	

1ï¼‰å°†mysql-connector-java-5.1.48.jar ä¸Šä¼ åˆ°/opt/softwareè·¯å¾„

2ï¼‰è¿›å…¥åˆ°/opt/software/è·¯å¾„ï¼Œæ‹·è´jdbcé©±åŠ¨åˆ°sqoopçš„libç›®å½•ä¸‹ã€‚

```
[atguigu@hadoop102 software]$ cp mysql-connector-java-5.1.48.jar /opt/module/sqoop/lib/
```

## 10.4 éªŒè¯Sqoop	

ï¼ˆ1ï¼‰æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸä¸€ä¸ªcommandæ¥éªŒè¯sqoopé…ç½®æ˜¯å¦æ­£ç¡®ï¼š

```
[atguigu@hadoop102 sqoop]$ bin/sqoop help
```

ï¼ˆ2ï¼‰å‡ºç°ä¸€äº›Warningè­¦å‘Šï¼ˆè­¦å‘Šä¿¡æ¯å·²çœç•¥ï¼‰ï¼Œå¹¶ä¼´éšç€å¸®åŠ©å‘½ä»¤çš„è¾“å‡ºï¼š

```
Available commands:
  codegen            Generate code to interact with database records
  create-hive-table     Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables     Import tables from a database to HDFS
  import-mainframe    Import datasets from a mainframe server to HDFS
  job                Work with saved jobs
  list-databases        List available databases on a server
  list-tables           List available tables in a database
  merge              Merge results of incremental imports
  metastore           Run a standalone Sqoop metastore
  version            Display version information
```

## 10.5 æµ‹è¯•Sqoopæ˜¯å¦èƒ½å¤ŸæˆåŠŸè¿æ¥æ•°æ®åº“	

```
[atguigu@hadoop102 sqoop]$ bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password fgl123
```

å‡ºç°å¦‚ä¸‹è¾“å‡ºï¼š

```
information_schema
metastore
mysql
oozie
performance_schema
```

## 10.6 SqoopåŸºæœ¬ä½¿ç”¨	

å°†mysqlä¸­user_infoè¡¨æ•°æ®å¯¼å…¥åˆ°HDFSçš„/testè·¯å¾„

```
bin/sqoop import \
--connect jdbc:mysql://hadoop102:3306/gmall \
--username root \
--password fgl123 \
--table user_info \
--columns id,login_name \
--where "id>=10 and id<=30" \
--target-dir /test \
--delete-target-dir \
--fields-terminated-by '\t' \
--num-mappers 2 \
--split-by id
```

# 11.0ğŸ˜ƒSpark-yarnæ¨¡å¼éƒ¨ç½²



â€‹		**ç‹¬ç«‹éƒ¨ç½²ï¼ˆStandaloneï¼‰æ¨¡å¼ç”± Spark è‡ªèº«æä¾›è®¡ç®—èµ„æºï¼Œæ— éœ€å…¶ä»–æ¡†æ¶æä¾›èµ„æºã€‚è¿™ç§æ–¹å¼é™ä½äº†å’Œå…¶ä»–ç¬¬ä¸‰æ–¹èµ„æºæ¡†æ¶çš„è€¦åˆæ€§ï¼Œç‹¬ç«‹æ€§éå¸¸å¼ºã€‚ä½†æ˜¯ä½ ä¹Ÿè¦è®°ä½ï¼ŒSpark ä¸»è¦æ˜¯è®¡ç®—æ¡†æ¶ï¼Œè€Œä¸æ˜¯èµ„æºè°ƒåº¦æ¡†æ¶ï¼Œæ‰€ä»¥æœ¬èº«æä¾›çš„èµ„æºè°ƒåº¦å¹¶ä¸æ˜¯å®ƒçš„å¼ºé¡¹ï¼Œæ‰€ä»¥è¿˜æ˜¯å’Œå…¶ä»–ä¸“ä¸šçš„èµ„æºè°ƒåº¦æ¡†æ¶é›†æˆä¼šæ›´é è°±ä¸€äº›ã€‚æ‰€ä»¥æ¥ä¸‹æ¥æˆ‘ä»¬æ¥å­¦ä¹ åœ¨å¼ºå¤§çš„ Yarn ç¯å¢ƒä¸‹ Spark æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼ˆå…¶å®æ˜¯å› ä¸ºåœ¨å›½å†…å·¥ä½œä¸­ï¼ŒYarn ä½¿ç”¨çš„éå¸¸å¤šï¼‰ã€‚**

## 11.1 è§£å‹ç¼©æ–‡ä»¶

å°† spark-3.0.0-bin-hadoop3.2.tgz æ–‡ä»¶ä¸Šä¼ åˆ° linux å¹¶è§£å‹ç¼©ï¼Œæ”¾ç½®åœ¨æŒ‡å®šä½ç½®ã€‚

```
tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
cd /opt/module 
mv spark-3.0.0-bin-hadoop3.2 spark-yarn
```

## 11.2 ä¿®æ”¹é…ç½®æ–‡ä»¶

1) ä¿®æ”¹ hadoop é…ç½®æ–‡ä»¶/opt/module/hadoop/etc/hadoop/yarn-site.xml, å¹¶åˆ†å‘

```
<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„ç‰©ç†å†…å­˜é‡ï¼Œå¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼ï¼Œåˆ™ç›´æ¥å°†å…¶æ€æ‰ï¼Œé»˜è®¤æ˜¯ true -->
<property>
 <name>yarn.nodemanager.pmem-check-enabled</name>
 <value>false</value>
</property>
<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„è™šæ‹Ÿå†…å­˜é‡ï¼Œå¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼ï¼Œåˆ™ç›´æ¥å°†å…¶æ€æ‰ï¼Œé»˜è®¤æ˜¯ true -->
<property>
 <name>yarn.nodemanager.vmem-check-enabled</name>
 <value>false</value>
</property>
```

2) ä¿®æ”¹ conf/spark-env.shï¼Œæ·»åŠ  JAVA_HOME å’Œ YARN_CONF_DIR é…ç½®

```
 spark-env.sh.template spark-env.sh

export JAVA_HOME=/opt/module/jdk1.8.0_212
YARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop
```

## 11.3 å¯åŠ¨ HDFS ä»¥åŠ YARN é›†ç¾¤

è‡ªå·±å¯åŠ¨hadoopğŸ˜ƒ

## 11.4 æäº¤åº”ç”¨

```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
```

æŸ¥çœ‹ http://linux2:8088 é¡µé¢ï¼Œç‚¹å‡» Historyï¼ŒæŸ¥çœ‹å†å²é¡µé¢

![1659444089612](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1659444089612.png)

## 11.5 é…ç½®å†å²æœåŠ¡å™¨

3.3.5 é…ç½®å†å²æœåŠ¡å™¨ 

1) ä¿®æ”¹ spark-defaults.conf.template æ–‡ä»¶åä¸º spark-defaults.conf

```
mv spark-defaults.conf.template spark-defaults.conf
```

2) ä¿®æ”¹ spark-default.conf æ–‡ä»¶ï¼Œé…ç½®æ—¥å¿—å­˜å‚¨è·¯å¾„

```
spark.eventLog.enabled true
spark.eventLog.dir hdfs://hadoop102:8020/directory
```

æ³¨æ„ï¼šéœ€è¦å¯åŠ¨ hadoop é›†ç¾¤ï¼ŒHDFS ä¸Šçš„ç›®å½•éœ€è¦æå‰å­˜åœ¨ã€‚

```
[root@linux1 hadoop]# sbin/start-dfs.sh
[root@linux1 hadoop]# hadoop fs -mkdir /directory
```

3) ä¿®æ”¹ spark-env.sh æ–‡ä»¶, æ·»åŠ æ—¥å¿—é…ç½®

```
export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080 
-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/directory 
-Dspark.history.retainedApplications=30"
```

âš« å‚æ•° 1 å«ä¹‰ï¼šWEB UI è®¿é—®çš„ç«¯å£å·ä¸º 18080
âš« å‚æ•° 2 å«ä¹‰ï¼šæŒ‡å®šå†å²æœåŠ¡å™¨æ—¥å¿—å­˜å‚¨è·¯å¾„
âš« å‚æ•° 3 å«ä¹‰ï¼šæŒ‡å®šä¿å­˜ Application å†å²è®°å½•çš„ä¸ªæ•°ï¼Œå¦‚æœè¶…è¿‡è¿™ä¸ªå€¼ï¼Œæ—§çš„åº”ç”¨ç¨‹åº
ä¿¡æ¯å°†è¢«åˆ é™¤ï¼Œè¿™ä¸ªæ˜¯å†…å­˜ä¸­çš„åº”ç”¨æ•°ï¼Œè€Œä¸æ˜¯é¡µé¢ä¸Šæ˜¾ç¤ºçš„åº”ç”¨æ•°ã€‚

4) ä¿®æ”¹ spark-defaults.conf

```
spark.yarn.historyServer.address=hadoop102:18080
spark.history.ui.port=18080
```

5) å¯åŠ¨å†å²æœåŠ¡

```
sbin/start-history-server.sh 
```

6) é‡æ–°æäº¤åº”ç”¨

```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
```

7) Web é¡µé¢æŸ¥çœ‹æ—¥å¿—ï¼šhttp://hadoop103:8088

![1659444177150](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/1659444177150.png)
